


# Core System Architecture

이번 챕터에서는 core system 의 아키텍처 설계 및 변천사를 소개합니다.



### Microservice Architecture

가장 먼저, worker 은 아키텍처 그림에서도 한가운데를 차지하며 가장 많은 인프라나 어플리케이션과 상호작용하고 있는 만큼 전체 Goboolean 서비스에서 가장 핵심적인 기능이라고 할 수 있습니다. Worker 은 모델을 소스코드의 형태로 가져와 실행하고, 방대한 양의 과거 데이터를 가져와 이를 모델에 인풋으로 제공하면, 모델은 그에 상응하는 양의 계산을 진행합니다. 하나의 모델에 대해 여러 파라미터의 경우를 동시에 테스트하는 경우 그만큼 수 배의 계산을 해야 합니다. 이러한 특징으로 인해 Worker 가 단 하나의 Task 를 처리하더라도 프로세스가 쉬는 일 없이 사용 가능한  CPU를 온전히 소모하게 됩니다.

Worker 가 다른 서비스에 영향을 주지 않게 하기 위해서 Worker 는 다른 서비스와 분리되어야 합니다. Worker 와 격리되지 않은 환경에서 동작하는 어플리케이션은 Worker 와 자원을 공유합니다. 이 경우 Worker 에 의해 다른 서비스가 느려져 쾌적하지 않은 사용자 경험을 제공하게 될 수 있습니다. CPU 사용률이 100%을 넘기는 경우 서비스는 이에 비례하여 급격하게 느려집니다. 100%를 넘기지 않더라도 서비스의 Latency 에 여전히 큰 영향을 줄 수 있습니다. 쾌적한 속도를 위해 권장되는 CPU 사용량 상한이 어느 정도인지는 잘 모르지만, 이에 비해서는 한참 낮은 편일 것입니다. CPU 를 혹사(?)시켜도 되는 Worker 와 Latency가 가장 중요한 core system 의 서비스들을 서로 다른 어플리케이션으로 격리하는 것이 적절합니다.



### Worker Scheduling

워커의 작업에 부하가 상당히 큰 만큼 조금만 더 최적화를 진행하기로 하였습니다. 처음으로 주목한 부분은 Context Switching 비용을 최소화하는 것입니다. Context Switching 은 여러 프로세스가 하나의 CPU 위에서 동작할때 CPU 는 여러 프로세스를 번갈아가며 실행하는데, 이때 프로세스를 바꾸며 발생하는 비용입니다. 일반적으로 이 비용은 프로세스의 개수와 비례하여 커집니다.

모놀리틱한 개념에서 많은 자원을 점유하는 worker server 를 하나 띄우고, 요청을 받는 대로 작업을 실행하는 방법을 가장 먼저 떠올릴 수 있습니다. 이 경우 사용자의 요청이 몰려 수십개의 요청을 한꺼번에 처리해야 하는 경우 컨텍스트 스위칭 비용이 크게 늘어날 수 있습니다. 이와 비교하여 하나의 CPU만 점유하는 worker 를 여러 개 띄우고, 하나의 worker 은 하나의 task 만 담당하고, 나머지는 대기시키는 스케줄링을 진행할경우 최종적으로 모든 작업을 더 빠르게 처리할 수 있습니다. 이때는 작업 실행의 시간이 부하 수준과 무관하게 일정하게 소모된다는 서비스상의 이점도 있습니다.

kubernetes 의 deployments 를 통해 이를 띄워 위 아이디어를 실현하였습니다. 이 경우 하나의 pod 에서 하나의 worker 가 실행되어 하나의 task 를 담당하게 되고, 미리 설정된 값 만큼 worker pod 이 띄워져 서비스됩니다. 이에는 운영상의 이점도 있습니다. 부하가 늘어나는 경우 동적으로 replica 의 개수를 늘이거나 새로운 worker cluster 을 띄워 유연하게 대응할 수 있습니다.

설계 과정에서 의도한 바는 kubernetes deployment 를 통해 배포하는 과정에서 kubernetes 가 자원을 할당하며 자체적으로 pod 과 cpu 를 매핑하는 것을 기대하거나 설정을 통해 하나의 CPU 가 하나의 프로세스만 담당하도록 하여 컨텍스트 스위칭 비용을 최대한으로 줄이는 것입니다. 다만 이것이 실제로 가능한지에 대해서는 조사가 더 필요합니다.

스케줄링을 위해서는 worker queue 와 task queue 를 두어, 둘을 서로 매치시켜 주어야 합니다. 저희 서비스에서는 큐를 어플리케이션 수준에서 구현하지 않고 인프라 수준에서 그 개념을 표현하는 수준으로 구현하였습니다. kafka 를 task queue 로, kubernetes replica set 을 worker queue 로 두어, pods 의 인스턴스가 kafka 를 구독하는 방식으로 자연스럽게 스케줄링이 가능하도록 하였습니다. 이렇게 큐를 서비스 수준에서 구현하지 않음으로써 인스턴스간 결합도를 낮추는 방법을 제안하였지만, queue 를 튜닝할 수 없다는 점이 단점이어서 검토가 좀 더 필요합니다.


### Event Driven Architecture

core system 은 command, worker, join 으로 이루어진 마이크로서비스입니다. 각각의 인스턴스를 event 에 반응하여 적절하게 처리하는 서비스로 두고, 필요한 이벤트를 구독하여 처리하도록 하였습니다. 이벤트를 다루는 메시지 큐로는 kafka 를 활용하였습니다.

Event Driven 은 구독자가 게시자에 관한 정보를 알 필요가 없고 이벤트에만 의존하면 되어 서비스간 결합도를 낮출 수 있는데 장점이 있습니다. 이로 인해 구독자가 각각 독립적으로 동일한 데이터를 구독하는 것도 가능하게 됩니다. 저희 서비스에서는 특별히 worker 이 발행하는 이벤트를 각기 다른 목적으로 command 와 join 이 구독해야 하는데, 해당 아키텍처 도입을 통해 이를 해결하였습니다.



### CQRS Pattern

부하가 큰 Worker 을 독립된 컴포넌트로 떼어내었다면, 그 다음으로는 Worker 와 상호작용하는 다른 요소들을 어떻게 배치할지 생각하여야 합니다.

Worker 을 둘러싼 중요한 도메인 로직들은 아래와 같습니다.
1. 스케줄링을 통해 Task 를 대기시키고 Worker 에 할당
2. Worker 에 데이터를 제공
3. 분석 결과를 데이터와 join 하여 사용자에게 제공
4. 분석 결과를 구독하여 자동 거래를 진행

CQRS(Command Query Reliability Segregation) 이라 불리는 한 가지 아키텍처 패턴이 있습니다. 이 패턴은 하나의 도메인을 서비스 도메인과 쿼리 도메인으로 쪼개 서로 다른 인스턴스로 분리하는 패턴입니다. 앞서 소개한 도메인 로직을 이에 따라 구분하여 독립된 인스턴스로 쪼개었습니다.
* 2번은 fetch system 의 역할이기 때문에 core system 에서 제외됩니다.
* 1번과 4번 그리고 worker 이 command 의 책임과 잘 어울립니다. 앞서 소개한 것과 같은 이유로 worker 은 독립된 서비스로 구분하였으나, 다만 1번과 4번은 서로 독립된 서비스로 쪼개야 할 중요한 이유가 없다고 보았기 때문에 command 라는 하나의 서비스에 포함시켰습니다.
* 3번은 분석 결과와 데이터를 읽어 사용자에게 제공하기 때문에 query 의 책임과 잘 어울립니다. 이에 command 와 독립된 서비스로 join 을 두었습니다.

이렇게 하여 core system 은 task 를 일대일로 처리하는 worker, 주요한 도메인 로직을 담당하는 command, 결과의 쿼리를 담당하는 join 으로 구성이 됩니다.


마이크로서비스에서는 각 인스턴스가 해당 도메인과 잘 어울리는 DB 를 선택할 수 있습니다. 이에 Command 책임을 갖는 서비스는 조인  관계가 복잡한 도메인의 표현을 위해 PostgreSQL 을 사용하였고, Join 책임을 갖는 서비스는 단순한 데이터의 표현과 빠른 쿼리를 위해 MongoDB 를 사용하였습니다.

일반적인 CQRS 패턴에서는 어떤 이벤트가 발생하면 Command 와 Query 에 책임이 있는 서비스가 각각 자신의 도메인에 맞는 방법으로 데이터를 처리하고 저장합니다. 다만 이때 두 도메인의 데이터에 일관성만 갖춰진다면 각 서비스에서 필요로 하는 데이터만 저장을 하여도 무방합니다. core system 에서는 worker 이 계속해서 분석 결과 이벤트를 발생시킬 때, Join 서버에서는 데이터의 내용을 저장하고, Command 에서는 메타데이터를 저장하였습니다.

어떤 두 서비스의 부하 수준이 다를 때 두 서비스를 독립된 서버로 분리하면 서비스별 유연한 확장이 가능하여 마이크로서비스의 장점을 살릴 수 있습니다. 일반적인 서비스에서는 Query 에 관한 요청이 Command 에 관한 요청에 비해 훨씬 많은 트래픽을 차지하기 때문에 CQRS 패턴을 도입하는 경우가 많습니다. 저희 서비스는 Query 에 해당하는 요청의 비율이 높다고 보기도 어려우며, 설계 과정에서 많은 트래픽에 대응하는 것을 주요한 목적으로 삼지는 않았습니다. 다만 Query 는 매우 방대한 양의 데이터를 읽기 때문에 사용자에게 stream 으로 데이터를 제공하며 이에 따라 하나의 요청의 부하가 큰 편이라고 할 수 있습니다. 이 점에서 MSA 의 목적과 부합하는 측면이 있습니다.
